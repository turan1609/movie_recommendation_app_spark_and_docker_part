{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911db214-fcc5-4925-95a9-9b3651eafd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CHECKPOINT 0: KÜTÜPHANELERİN YÜKLENMESİ VE SPARK OTURUMUNUN BAŞLATILMASI\n",
    "# ==============================================================================\n",
    "print(\">>> CHECKPOINT 0: Kütüphaneler yükleniyor ve Spark başlatılıyor...\")\n",
    "\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, udf, collect_list\n",
    "from pyspark.sql.types import IntegerType, FloatType, StringType, StructType, StructField\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# SparkSession'ı, güçlü bir makineye uygun ve geçici dosya sorununu önleyecek şekilde yapılandıralım\n",
    "# HOME environment değişkeninden kullanıcı adını alalım\n",
    "home_dir = os.path.expanduser('~')\n",
    "spark_temp_dir = os.path.join(home_dir, \"spark-temp\")\n",
    "spark_checkpoint_dir = os.path.join(home_dir, \"spark-checkpoints\")\n",
    "\n",
    "# Geçici ve checkpoint dizinleri yoksa oluşturalım\n",
    "os.makedirs(spark_temp_dir, exist_ok=True)\n",
    "os.makedirs(spark_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"ALS_Model_Optimization\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")  # Lab makinesi için belleği artıralım\n",
    "    .config(\"spark.executor.memory\", \"16g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .config(\"spark.local.dir\", spark_temp_dir) # Geçici dosyalar için /home altında bir dizin\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "sc.setCheckpointDir(spark_checkpoint_dir) # Uzun süren işlemlerde hata toleransı için\n",
    "\n",
    "print(f\"SparkSession başlatıldı. Spark Sürümü: {spark.version}\")\n",
    "print(f\"Geçici Spark dosyaları için kullanılacak dizin: {spark_temp_dir}\")\n",
    "print(f\"Checkpoint dizini: {spark_checkpoint_dir}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CHECKPOINT 1: VERİLERİN YÜKLENMESİ VE ÖNBELLEĞE ALINMASI\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> CHECKPOINT 1: Veriler yükleniyor...\")\n",
    "\n",
    "BASE_PATH = \"./\"\n",
    "MOVIES_PATH = f\"{BASE_PATH}movies.csv\"\n",
    "RATINGS_PATH = f\"{BASE_PATH}ratings.csv\"\n",
    "GENOME_SCORES_PATH = f\"{BASE_PATH}genome-scores.csv\"\n",
    "\n",
    "# Şemalar\n",
    "movie_schema = StructType([\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), True)\n",
    "])\n",
    "\n",
    "rating_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "genome_scores_schema = StructType([\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"tagId\", IntegerType(), True),\n",
    "    StructField(\"relevance\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Veri Yükleme\n",
    "movies_df = spark.read.csv(MOVIES_PATH, header=True, schema=movie_schema, escape='\"')\n",
    "ratings_df_raw = spark.read.csv(RATINGS_PATH, header=True, schema=rating_schema)\n",
    "genome_scores_df = spark.read.csv(GENOME_SCORES_PATH, header=True, schema=genome_scores_schema)\n",
    "\n",
    "# DataFrame'leri önbelleğe al\n",
    "movies_df.cache()\n",
    "ratings_df_raw.cache()\n",
    "genome_scores_df.cache()\n",
    "\n",
    "print(f\"  - Filmler yüklendi: {movies_df.count()} adet\")\n",
    "print(f\"  - Derecelendirmeler yüklendi: {ratings_df_raw.count()} adet\")\n",
    "print(f\"  - Genom skorları yüklendi: {genome_scores_df.count()} adet\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CHECKPOINT 2: VERİ ÖN İŞLEME\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> CHECKPOINT 2: Veri ön işleme adımları...\")\n",
    "\n",
    "# --- ALS için Veri Hazırlığı ---\n",
    "als_ratings_df = ratings_df_raw.select(\n",
    "    col(\"userId\").alias(\"user\"),\n",
    "    col(\"movieId\").alias(\"item\"),\n",
    "    \"rating\"\n",
    ").cache()\n",
    "print(\"  - ALS için veri hazırlığı tamamlandı.\")\n",
    "\n",
    "# --- Kosinüs Benzerliği için Zenginleştirilmiş Veri Hazırlığı (TAG GENOME) ---\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(sorted(l)), VectorUDT())\n",
    "\n",
    "movies_tag_genome_df = (genome_scores_df\n",
    "                        .groupBy(\"movieId\")\n",
    "                        .agg(collect_list(\"relevance\").alias(\"relevance_list\"))\n",
    "                        .orderBy(\"movieId\"))\n",
    "\n",
    "movies_featured_df_raw = movies_tag_genome_df.withColumn(\n",
    "    \"features\",\n",
    "    list_to_vector_udf(col(\"relevance_list\"))\n",
    ").select(\"movieId\", \"features\")\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\")\n",
    "movies_featured_df = normalizer.transform(movies_featured_df_raw).selectExpr(\"movieId\", \"normFeatures as features\").cache()\n",
    "\n",
    "print(\"  - Kosinüs benzerliği için 'Tag Genome' tabanlı özellik vektörleri oluşturuldu.\")\n",
    "\n",
    "# --- Kosinüs Benzerliği Fonksiyonu ---\n",
    "def find_similar_movies(movie_id, top_n=10):\n",
    "    try:\n",
    "        target_vector_row = movies_featured_df.filter(col(\"movieId\") == movie_id).first()\n",
    "        if not target_vector_row:\n",
    "            print(f\"Uyarı: {movie_id} ID'li film için özellik vektörü bulunamadı.\")\n",
    "            return None\n",
    "        target_vector = target_vector_row.features\n",
    "        \n",
    "        dot_product_udf = udf(lambda x: float(x.dot(target_vector)), FloatType())\n",
    "        \n",
    "        similarities_df = movies_featured_df.withColumn(\"similarity\", dot_product_udf(col(\"features\")))\n",
    "        \n",
    "        top_similar_movies = (similarities_df\n",
    "                              .filter(col(\"movieId\") != movie_id)\n",
    "                              .orderBy(col(\"similarity\").desc())\n",
    "                              .limit(top_n)\n",
    "                              .join(movies_df, \"movieId\"))\n",
    "        return top_similar_movies\n",
    "    except Exception as e:\n",
    "        print(f\"Benzer filmler bulunurken hata: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"  - 'find_similar_movies' fonksiyonu tanımlandı.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CHECKPOINT 3: HİPERPARAMETRE OPTİMİZASYONU İLE EN İYİ ALS MODELİNİ BULMA\n",
    "# ==============================================================================\n",
    "print(\"\\n>>> CHECKPOINT 3: ALS modeli için hiperparametre optimizasyonu...\")\n",
    "\n",
    "# --- Veri Setini Ayırma ---\n",
    "(training_df, test_df) = als_ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "training_df.cache()\n",
    "test_df.cache()\n",
    "print(f\"  - Eğitim seti boyutu: {training_df.count()}\")\n",
    "print(f\"  - Test seti boyutu: {test_df.count()}\")\n",
    "\n",
    "# --- Optimizasyon Ayarları ---\n",
    "als = ALS(\n",
    "    userCol=\"user\",\n",
    "    itemCol=\"item\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(als.rank, [12, 20, 30, 50])\n",
    "              .addGrid(als.regParam, [0.1, 0.5, 1.0])\n",
    "              .addGrid(als.maxIter, [15, 20])\n",
    "              .build()\n",
    "             )\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3, # 3 katlı çapraz doğrulama\n",
    "    parallelism=4, # Aynı anda 4 modeli paralel olarak deneyebilir\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# --- Optimizasyonu Başlat ---\n",
    "print(f\"\\n  - Optimizasyon başlıyor. Toplam {len(param_grid)} model denenecek...\")\n",
    "print(\"  - BU İŞLEM UZUN SÜRECEKTİR...\")\n",
    "\n",
    "start_time = time.time()\n",
    "cv_model = cross_validator.fit(training_df)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n  - Optimizasyon ve eğitim { (end_time - start_time) / 60:.2f} dakikada tamamlandı.\")\n",
    "\n",
    "# --- En İyi Modeli ve Sonuçları Göster ---\n",
    "als_model = cv_model.bestModel\n",
    "\n",
    "print(\"\\n--- EN İYİ MODEL SONUÇLARI ---\")\n",
    "print(f\"  - En İyi Rank: {als_model.rank}\")\n",
    "print(f\"  - En İyi MaxIter: {als_model.getMaxIter()}\")\n",
    "print(f\"  - En İyi RegParam: {als_model.getRegParam():.4f}\")\n",
    "\n",
    "# --- Final Değerlendirme ---\n",
    "print(\"\\n  - En iyi modelin final performansı değerlendiriliyor...\")\n",
    "\n",
    "# Eğitim Seti Performansı\n",
    "train_predictions = als_model.transform(training_df)\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "print(f\"  - Eğitim Seti RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "# Test Seti Performansı\n",
    "test_predictions = als_model.transform(test_df)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(f\"  - Test Seti RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Overfitting Analizi\n",
    "overfitting_ratio = (test_rmse - train_rmse) / train_rmse * 100\n",
    "print(f\"\\n  - Test hatası, eğitim hatasından %{overfitting_ratio:.2f} daha yüksek.\")\n",
    "if overfitting_ratio > 10:\n",
    "    print(\"  - DİKKAT: Modelde overfitting (aşırı öğrenme) eğilimi olabilir.\")\n",
    "else:\n",
    "    print(\"  - Modelin genelleme performansı iyi görünüyor.\")\n",
    "\n",
    "print(\"\\n>>> MODEL OPTİMİZASYONU TAMAMLANDI. 'als_model' değişkeni en iyi modeli içermektedir.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Bundan sonraki hücrelerde 'als_model' değişkenini kullanarak\n",
    "# interaktif tavsiye sistemini çalıştırabilirsiniz.\n",
    "# =============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
